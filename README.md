# Assessing Novelty in LLMs

This project provides a **comprehensive suite of benchmarks** for evaluating the creative capabilities of Large Language Models (LLMs) across multiple domains:

- ✍️ Poetry  
- 🎭 Humor  
- 🎲 Game Building  
- 📖 Prose 

Each benchmark is designed to assess **originality**, **novelty**, and **semantic divergence** using techniques such as:

- Graph-based similarity
- Embedding distance
- LLM-based originality scoring (GPT-4o)

---

## Project Structure

Each benchmark is self-contained in its own folder with dedicated code, data, and results. Navigate into each folder to run the corresponding analysis and explore the evaluation logic.


---

Each subfolder contains its own `README.md` with details on:

- Dataset structure
- Scoring methodology
- Dependencies
- Output format
- Visualizations

> Please refer to each benchmark directory for detailed instructions on running that evaluation.

---

